{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team, and modifications by me.</center>","metadata":{}},{"cell_type":"markdown","source":"# Training Notebook\n\n# RSNA 2023 Abdominal Trauma Detection with [KerasCV](https://github.com/keras-team/keras-cv) and [KerasCore](https://github.com/keras-team/keras-core)\n\nThis notebook walks you through how to train a **Convolutional Neural Network (CNN)** model using Keras (Core and CV) on the RSNA 2023 Abdominal Trauma Detection dataset made available for this competition.\n\nFun fact: This notebook is backend (tensorflow, pytorch, jax) agnostic. Using KerasCV and KerasCore we can choose a backend of our choise! Feel free to read [Keras Core](https://keras.io/keras_core/announcement/) announcement to know more about Keras.\n\nIn this notebook you will learn:\n\n* Loading the data using [`tf.data`](https://www.tensorflow.org/guide/data).\n* Applying augmentations inside the data pipeline.\n* Create the model using KerasCV presets.\n* Train the model.\n* Visualize the training plots.\n\n## Notebooks\n\nFor this competition we have two starter notebook. This notebook (you are reading) trains the model on the dataset, while there lies another notebook that performs inference and submits to the competition.\n\n1. [**Training Kernel**](https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-train)\n2. [**Inference Kernel**](https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-infer)\n\n**Note**: [KerasCV guides](https://keras.io/guides/keras_cv/) is the place to go for a deeper understanding of KerasCV individually.","metadata":{}},{"cell_type":"markdown","source":"# Setup and Imports\n\nWe will need KerasCV for this notebook.\n\nFeel free to use `pip install keras-cv` instead of the installation from github.","metadata":{}},{"cell_type":"code","source":"! pip install --upgrade pip\n# ! pip install keras-cv\n! pip install seaborn\n! pip install boto3","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# You can use `tensorflow`, `pytorch`, `jax` here\n# KerasCore makes the notebook backend agnostic :)\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras_cv\nimport keras_core as keras\nfrom keras_core import layers\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import BatchNormalization\n\n# Utils\nimport gc\n\n# plots\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# aws\nimport boto3\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration\n\nA particularly good practise is to have a configuration class for your notebooks. This not only keeps your configurations all at a single place but also becomes handy to map the configs to the performance of the model.\n\nPlease play around with the configurations and see how the performance of the model changes.","metadata":{}},{"cell_type":"code","source":"class Config:\n    SEED = 42\n    IMAGE_SIZE = [256, 256]\n    BATCH_SIZE = 64\n    EPOCHS = 10\n    TARGET_COLS  = [\n        \"bowel_injury\", \"extravasation_injury\",\n        \"kidney_healthy\", \"kidney_low\", \"kidney_high\",\n        \"liver_healthy\", \"liver_low\", \"liver_high\",\n        \"spleen_healthy\", \"spleen_low\", \"spleen_high\",\n    ]\n    AUTOTUNE = tf.data.AUTOTUNE\n\nconfig = Config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reproducibility\n\nWe would want this notebook to have reproducible results. Here we set the seed for all the random algorithms so that we can reproduce the experiments each time exactly the same way.","metadata":{}},{"cell_type":"code","source":"keras.utils.set_random_seed(seed=config.SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"# Dataset\n\nThe dataset provided in the competition consists of DICOM images. We will not be training on the DICOM images, rather would work on PNG image which are extracted from the DICOM format.\n\n[A helpful resource on the conversion of DICOM to PNG](https://www.kaggle.com/code/radek1/how-to-process-dicom-images-to-pngs)","metadata":{}},{"cell_type":"code","source":"BASE_PATH = f\"/kaggle/input/rsna-atd-512x512-png-v2-dataset\"","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Meta Data\n\nThe `train.csv` file contains the following meta information:\n\n- `patient_id`: A unique ID code for each patient.\n- `series_id`: A unique ID code for each scan.\n- `instance_number`: The image number within the scan. The lowest instance number for many series is above zero as the original scans were cropped to the abdomen.\n- `[bowel/extravasation]_[healthy/injury]`: The two injury types with binary targets.\n- `[kidney/liver/spleen]_[healthy/low/high]`: The three injury types with three target levels.\n- `any_injury`: Whether the patient had any injury at all.\n","metadata":{}},{"cell_type":"code","source":"# train\ndataframe = pd.read_csv(f\"{BASE_PATH}/train.csv\")\ndataframe[\"image_path\"] = f\"{BASE_PATH}/train_images\"\\\n                    + \"/\" + dataframe.patient_id.astype(str)\\\n                    + \"/\" + dataframe.series_id.astype(str)\\\n                    + \"/\" + dataframe.instance_number.astype(str) +\".png\"\ndataframe = dataframe.drop_duplicates()\n\ndataframe.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the summary statistics of the dataset\nsummary_statistics = dataframe.describe()\n\nsummary_statistics.iloc[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for missing values\nmissing_values = dataframe.isnull().sum()\n\nmissing_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing values makes it easier for us!","metadata":{}},{"cell_type":"code","source":"del missing_values\ndel summary_statistics\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class Dependency\n\nWe will explore the difference between health columns vs. injury columns","metadata":{}},{"cell_type":"code","source":"# Selecting columns related to the health of organs\nhealth_columns = [\n    \"bowel_healthy\", \"extravasation_healthy\", \"kidney_healthy\", \n    \"liver_healthy\", \"spleen_healthy\",\n]\n\n# Calculating the correlation matrix for the selected columns\ncorrelation_matrix = dataframe[health_columns].corr()\n\n# Plotting the heatmap to visualize the correlations\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"YlGnBu\", linewidths=.5)\nplt.title(\"Correlation Heatmap of Organ Health\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlations between different health columns are generally small, indicating that the healthy state of one organ might not be strongly related to the healthy state of other organs.\n\nThere is no strong correlation between any two specific health-related columns, suggesting that the health status of different organs is relatively independent of each other.","metadata":{}},{"cell_type":"code","source":"# Selecting columns related to the health of organs\ninjury_columns = [\n    \"bowel_injury\", \"extravasation_injury\",\n    \"kidney_low\", \"kidney_high\", \n    \"liver_low\", \"liver_high\",\n    \"spleen_low\", \"spleen_high\",\n    \"any_injury\"\n]\n\n# Calculating the correlation matrix for the selected columns\ncorrelation_matrix = dataframe[injury_columns].corr()\n\n# Plotting the heatmap to visualize the correlations\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"YlGnBu\", linewidths=.5)\nplt.title(\"Correlation Heatmap of Organ Injury\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bowel and Extravasation:\n\n* `bowel_injury`: This shows a moderate correlation with `any_injury` (0.24) and a smaller correlation with `extravasation_injury` (0.13)\n* `extravasation_injury`: This has a strong correlation with `any_injury` (0.43) and a moderate correlation with `spleen_high` (0.200).\n\n### Kidney:\n\n* `kidney_low`: This is moderately correlated with `ny_injury` (0.319).\n* `kidney_high`: Similar to kidney_low, this is moderately correlated with `any_injury` (0.24).\n\n### Liver:\n\n* `liver_low`: This has a strong correlation with `any_injury` (0.490).\n* `liver_high`: This shows a moderate correlation with `any_injury` (0.232).\n\n### Spleen:\n\n* `spleen_low`: This is moderately correlated with `any_injury` (0.425).\n* `spleen_high`: This shows a moderate correlation with `any_injury` (0.373) and `extravasation_injury` (0.200).\n\n### Conclusions: \n\n**The `any_injury` column is moderately to strongly correlated with all other injury columns, suggesting that it may be a summary measure of injury presence across different organs.**\n\nThere are some specific correlations between individual injury types, such as the correlation between `extravasation_injury` and `spleen_high`.\n\nThe correlations between the low and high levels of organ injuries (e.g., `kidney_low` and `kidney_high`) are generally lower, indicating that these might be independent conditions.\n\nThe correlation between different organs' injuries is generally low, which might suggest that injuries to different organs occur independently of each other.","metadata":{}},{"cell_type":"markdown","source":"## Note on some observations\n\n1. Class Dependencies: Refers to inherent relationships between classes in the analysis.\n2. Complementarity: `bowel_injury` and `bowel_healthy`, as well as `extravasation_injury` and `extravasation_healthy`, are perfectly complementary, with their sum always equal to 1.0.\n3. Simplification: For the model, only `{bowel/extravasation}_injury` will be included, and the corresponding healthy status can be calculated using a sigmoid function.\n4. Softmax: `{kidney/liver/spleen}_{healthy/low/high}` classifications are softmaxed, ensuring their combined probabilities sum up to 1.0 for each organ, simplifying the model while preserving essential information.","metadata":{}},{"cell_type":"code","source":"del correlation_matrix\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Wrangling","metadata":{}},{"cell_type":"markdown","source":"We split the training dataset into train and validation. This is a common practise in the Machine Learning pipelines. We not only want to train our model, but also want to validate it's training.\n\nA small catch here is that the training and validation data should have an aligned data distribution. Here we handle that by grouping the lables and then splitting the dataset. This ensures an aligned data distribution between the training and the validation splits.","metadata":{}},{"cell_type":"code","source":"# Function to handle the split for each group\ndef split_group(group, test_size=0.2):\n    if len(group) == 1:\n        return (group, pd.DataFrame()) if np.random.rand() < test_size else (pd.DataFrame(), group)\n    else:\n        return train_test_split(group, test_size=test_size, random_state=42)\n\n# Initialize the train and validation datasets\ntrain_data = pd.DataFrame()\nval_data = pd.DataFrame()\n\n# Iterate through the groups and split them, handling single-sample groups\nfor _, group in dataframe.groupby(config.TARGET_COLS):\n    train_group, val_group = split_group(group)\n    train_data = pd.concat([train_data, train_group], ignore_index=True)\n    val_data = pd.concat([val_data, val_group], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape, val_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pipeline /w tf.data\n\nHere we build the data pipeline using `tf.data`. Using `tf.data` we can map out data to an augmentation pipeline simple by using the ` map` API.\n\nAdding augmentations to the data pipeline is as simple as adding a layer into the list of layers that the `Augmenter` processes.\n\nReference: https://keras.io/api/keras_cv/layers/augmentation/","metadata":{}},{"cell_type":"code","source":"def decode_image_and_label(image_path, label):\n    file_bytes = tf.io.read_file(image_path)\n    image = tf.io.decode_png(file_bytes, channels=3, dtype=tf.uint8)\n    image = tf.image.resize(image, config.IMAGE_SIZE, method=\"bilinear\")\n    image = tf.cast(image, tf.float32) / 255.0\n    \n    label = tf.cast(label, tf.float32)\n    #         bowel       fluid       kidney      liver       spleen\n    labels = (label[0:1], label[1:2], label[2:5], label[5:8], label[8:11])\n    \n    return (image, labels)\n\n\ndef apply_augmentation(images, labels):\n    augmenter = keras_cv.layers.Augmenter(\n        [\n            keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n            keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n            \n        ]\n    )\n    return (augmenter(images), labels)\n\n\ndef build_dataset(image_paths, labels):\n    ds = (\n        tf.data.Dataset.from_tensor_slices((image_paths, labels))\n        .map(decode_image_and_label, num_parallel_calls=config.AUTOTUNE)\n        .shuffle(config.BATCH_SIZE * 10)\n        .batch(config.BATCH_SIZE)\n        .map(apply_augmentation, num_parallel_calls=config.AUTOTUNE)\n        .prefetch(config.AUTOTUNE)\n    )\n    return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths  = train_data.image_path.tolist()\nlabels = train_data[config.TARGET_COLS].values\n\nds = build_dataset(image_paths=paths, labels=labels)\nimages, labels = next(iter(ds))\nimages.shape, [label.shape for label in labels]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No more customizing your plots by hand, KerasCV has your back ;)\nkeras_cv.visualization.plot_image_gallery(\n    images=images,\n    value_range=(0, 1),\n    rows=2,\n    cols=2,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model\n\nWe are going to load a pretrained model from the [list of avaiable backbones in KerasCV](https://keras.io/api/keras_cv/models/backbones/). We are using the `ResNetBackbone` as our backbone. The practise of using a pretrained model and finetuning it to a specific dataset is prevalent in the DL community.\n\nWe use the [Functional API](https://keras.io/guides/functional_api/) of Keras to build the model. The design of the model would be such that we input a single image and we get different heads for the various predictions we need (kidney, spleen...).\n\nWe have also added a Learning Rate scheduler for you to work with. When an athlete trains, the first step is always to warm up. We take a similar approach to training our models. We warm up with model where the learning rate increses from the initial LR to a higher LR. After the warmup stage we provide a decay algorithm (cosine here). A list of all the learning rate scheduler can be found [here](https://keras.io/api/optimizers/learning_rate_schedules/).","metadata":{}},{"cell_type":"code","source":"def build_model(warmup_steps, decay_steps):\n    # Define Input\n    inputs = keras.Input(shape=config.IMAGE_SIZE + [3,], batch_size=config.BATCH_SIZE)\n    \n    # Define Backbone\n    backbone = keras_cv.models.ResNetBackbone.from_preset(\"resnet50_imagenet\")\n    backbone.include_rescaling = False\n    x = backbone(inputs)\n    \n    # GAP to get the activation maps\n    gap = keras.layers.GlobalAveragePooling2D()\n    x = gap(x)\n\n    # Define 'necks' for each head\n    x_bowel = keras.layers.Dense(32, activation='silu')(x)\n    x_extra = keras.layers.Dense(32, activation='silu')(x)\n    x_liver = keras.layers.Dense(32, activation='silu')(x)\n    x_kidney = keras.layers.Dense(32, activation='silu')(x)\n    x_spleen = keras.layers.Dense(32, activation='silu')(x)\n\n    # Define heads\n    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel) # use sigmoid to convert predictions to [0-1]\n    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra) # use sigmoid to convert predictions to [0-1]\n    out_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver) # use softmax for the liver head\n    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney) # use softmax for the kidney head\n    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen) # use softmax for the spleen head\n    \n    # Concatenate the outputs\n    outputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n\n    # Create model\n    print(\"[INFO] Building the model...\")\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Cosine Decay\n    cosine_decay = keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-4,\n        decay_steps=decay_steps,\n        alpha=0.0,\n        warmup_target=1e-3,\n        warmup_steps=warmup_steps,\n    )\n\n    # Compile the model\n    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\n    loss = {\n        \"bowel\":keras.losses.BinaryCrossentropy(),\n        \"extra\":keras.losses.BinaryCrossentropy(),\n        \"liver\":keras.losses.CategoricalCrossentropy(),\n        \"kidney\":keras.losses.CategoricalCrossentropy(),\n        \"spleen\":keras.losses.CategoricalCrossentropy(),\n    }\n    metrics = {\n        \"bowel\":[\"accuracy\"],\n        \"extra\":[\"accuracy\"],\n        \"liver\":[\"accuracy\"],\n        \"kidney\":[\"accuracy\"],\n        \"spleen\":[\"accuracy\"],\n    }\n    print(\"[INFO] Compiling the model...\")\n    model.compile(\n        optimizer=optimizer,\n      loss=loss,\n      metrics=metrics\n    )\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"# get image_paths and labels\nprint(\"[INFO] Building the dataset...\")\ntrain_paths = train_data.image_path.values; train_labels = train_data[config.TARGET_COLS].values.astype(np.float32)\nvalid_paths = val_data.image_path.values; valid_labels = val_data[config.TARGET_COLS].values.astype(np.float32)\n\n# train and valid dataset\ntrain_ds = build_dataset(image_paths=train_paths, labels=train_labels)\nval_ds = build_dataset(image_paths=valid_paths, labels=valid_labels)\n\ntotal_train_steps = train_ds.cardinality().numpy() * config.BATCH_SIZE * config.EPOCHS\nwarmup_steps = int(total_train_steps * 0.10)\ndecay_steps = total_train_steps - warmup_steps\n\nprint(f\"{total_train_steps=}\")\nprint(f\"{warmup_steps=}\")\nprint(f\"{decay_steps=}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualising the learning rate scheduler","metadata":{}},{"cell_type":"code","source":"# visualise learning rate scheduler\ndef visualize_learning_rate(warmup_steps, decay_steps, initial_learning_rate=1e-4, alpha=0.0, epochs=config.EPOCHS):\n    \"\"\"\n    Visualize the learning rate over epochs given specific cosine decay parameters.\n\n    Parameters:\n        - warmup_steps: int, number of steps for the warmup phase\n        - decay_steps: int, number of steps to apply the decay function\n        - initial_learning_rate: float, initial learning rate before decay\n        - alpha: float, minimum learning rate value as a fraction of the initial_learning_rate\n        - warmup_target: float, target warmup learning rate\n        - epochs: int, number of epochs to visualize the learning rate for\n    \"\"\"\n    cosine_decay = keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-4,\n        decay_steps=decay_steps,\n        alpha=alpha,\n        warmup_target=1e-3,\n        warmup_steps=warmup_steps,\n    )\n\n    lrs = [cosine_decay(epoch*total_train_steps/config.EPOCHS).numpy() for epoch in range(epochs)]\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(epochs), lrs, marker='o')\n    plt.title('Learning Rate Schedule')\n    plt.xlabel('Epoch')\n    plt.ylabel('Learning Rate')\n    plt.grid(True)\n    plt.show()\n\n# Example usage\nwarmup_steps = int(total_train_steps * 0.10)\ndecay_steps = total_train_steps - warmup_steps\nvisualize_learning_rate(warmup_steps, decay_steps, epochs=config.EPOCHS)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model with \"model.fit\"","metadata":{}},{"cell_type":"code","source":"# # build the model\n# print(\"[INFO] Building the model...\")\n# model = build_model(warmup_steps, decay_steps)\n\n# # train\n# print(\"[INFO] Training...\")\n# history = model.fit(\n#     train_ds,\n#     epochs=config.EPOCHS,\n#     validation_data=val_ds,\n# )","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use TPU to speed up the training process, as we're working with tensorflow datasets.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Initialize TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Creation Inside Strategy Scope: \nwith strategy.scope():\n    # Your model creation code here, for example:\n    model = build_model(warmup_steps, decay_steps)  # Assuming your function from the previous code\n    \n# Data: Ensure your data is in a TPU-friendly format.\n# Example: If you're using tf.data.Dataset for your data, you should batch the data appropriately.\n\nhistory = model.fit(\n    train_ds,\n    epochs=config.EPOCHS,\n    validation_data=val_ds,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the training plots","metadata":{}},{"cell_type":"code","source":"# Create a 3x2 grid for the subplots\nfig, axes = plt.subplots(5, 1, figsize=(5, 15))\n\n# Flatten axes to iterate through them\naxes = axes.flatten()\n\n# Iterate through the metrics and plot them\nfor i, name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n    # Plot training accuracy\n    axes[i].plot(history.history[name + '_accuracy'], label='Training ' + name)\n    # Plot validation accuracy\n    axes[i].plot(history.history['val_' + name + '_accuracy'], label='Validation ' + name)\n    axes[i].set_title(name)\n    axes[i].set_xlabel('Epoch')\n    axes[i].set_ylabel('Accuracy')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The visualisations suggest that our model is overfitting, as the validation loss increased with increasing training epochs, while the training loss decreases. To resolve this issue, we can add regularisation techniques. There are several techniques:\n1. Add dropout layers in the model.\n2. Use BatchNormalization\n3. Add L2 / L1 regularisation\n4. Use early stopping\n5. Data Augmentation, which we already did.\n\nWe experiment with adding dropout layers and batch normalization layers.","metadata":{}},{"cell_type":"code","source":"# del val_data\n# del train_data\n# gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(warmup_steps, decay_steps):\n    # Define Input\n    inputs = keras.Input(shape=config.IMAGE_SIZE + [3,], batch_size=config.BATCH_SIZE)\n    \n    # Define Backbone\n    backbone = keras_cv.models.ResNetBackbone.from_preset(\"resnet50_imagenet\")\n    backbone.include_rescaling = False\n    x = backbone(inputs)\n    \n    # GAP to get the activation maps\n    gap = keras.layers.GlobalAveragePooling2D()\n    x = gap(x)\n\n    # Define 'necks' for each head\n    x_bowel = keras.layers.Dense(32)(x)\n    x_bowel = keras.layers.BatchNormalization()(x_bowel)\n    \n    x_extra = keras.layers.Dense(32)(x)\n    x_extra = keras.layers.BatchNormalization()(x_extra)\n    \n    x_liver = keras.layers.Dense(32)(x)\n    x_liver = keras.layers.BatchNormalization()(x_liver)\n    \n    x_kidney = keras.layers.Dense(32)(x)\n    x_kidney = keras.layers.BatchNormalization()(x_kidney)\n    \n    x_spleen = keras.layers.Dense(32)(x)\n    x_spleen = keras.layers.BatchNormalization()(x_spleen)\n    \n    # Add Activation layers\n    x_bowel = keras.layers.Activation('silu')(x_bowel)\n    x_extra = keras.layers.Activation('silu')(x_extra)\n    x_liver = keras.layers.Activation('silu')(x_liver)\n    x_kidney = keras.layers.Activation('silu')(x_kidney)\n    x_spleen = keras.layers.Activation('silu')(x_spleen)\n    \n    # Add dropout layers to reduce overfitting\n    dropout_rate = 0.2\n    x_bowel = keras.layers.Dropout(rate=dropout_rate)(x_bowel)\n    x_extra = keras.layers.Dropout(rate=dropout_rate)(x_extra)\n    x_liver = keras.layers.Dropout(rate=dropout_rate)(x_liver)\n    x_kidney = keras.layers.Dropout(rate=dropout_rate)(x_kidney)\n    x_spleen = keras.layers.Dropout(rate=dropout_rate)(x_spleen)\n\n    # Define heads\n    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel) # use sigmoid to convert predictions to [0-1]\n    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra) # use sigmoid to convert predictions to [0-1]\n    out_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver) # use softmax for the liver head\n    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney) # use softmax for the kidney head\n    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen) # use softmax for the spleen head\n    \n    # Concatenate the outputs\n    outputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n\n    # Create model\n    print(\"[INFO] Building the model...\")\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Cosine Decay\n    cosine_decay = keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=1e-4,\n        decay_steps=decay_steps,\n        alpha=0.0,\n        warmup_target=1e-3,\n        warmup_steps=warmup_steps,\n    )\n\n    # Compile the model\n    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\n    loss = {\n        \"bowel\":keras.losses.BinaryCrossentropy(),\n        \"extra\":keras.losses.BinaryCrossentropy(),\n        \"liver\":keras.losses.CategoricalCrossentropy(),\n        \"kidney\":keras.losses.CategoricalCrossentropy(),\n        \"spleen\":keras.losses.CategoricalCrossentropy(),\n    }\n    metrics = {\n        \"bowel\":[\"accuracy\"],\n        \"extra\":[\"accuracy\"],\n        \"liver\":[\"accuracy\"],\n        \"kidney\":[\"accuracy\"],\n        \"spleen\":[\"accuracy\"],\n    }\n    print(\"[INFO] Compiling the model...\")\n    model.compile(\n        optimizer=optimizer,\n      loss=loss,\n      metrics=metrics\n    )\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll also use early stopping to prevent overfitting.","metadata":{}},{"cell_type":"code","source":"# Model Creation Inside Strategy Scope: \nwith strategy.scope():\n    # Your model creation code here, for example:\n    model = build_model(warmup_steps, decay_steps)  # Assuming your function from the previous code\n    \n# Data: Ensure your data is in a TPU-friendly format.\n# Example: If you're using tf.data.Dataset for your data, you should batch the data appropriately.\n\nhistory = model.fit(\n    train_ds,\n    epochs=config.EPOCHS+15,\n    validation_data=val_ds,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a function to plot the ROC-AUC curve and Precision-recall curve","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# import gc\n# from tqdm import tqdm\n# from sklearn.metrics import roc_curve, auc, precision_recall_curve\n\n\n# def get_predictions_and_labels_for_test(test_df, model):Ã¥\n#     patient_ids = test_df[\"patient_id\"].unique()\n#     patient_preds = np.zeros(shape=(len(patient_ids), 2*2 + 3*3), dtype=\"float32\")\n#     actual_labels = []\n    \n#     for pidx, patient_id in tqdm(enumerate(patient_ids), total=len(patient_ids), desc=\"Patients \"):\n#         patient_df = test_df.query(\"patient_id == @patient_id\")\n#         actual_labels.extend(patient_df['label'].values)\n#         patient_paths = patient_df.image_path.tolist()\n        \n#         dtest = build_dataset(patient_paths)\n#         pred = model.predict(dtest)\n#         pred = np.concatenate(pred, axis=-1).astype(\"float32\")\n#         pred = np.mean(pred.reshape(1, len(patient_paths), 11), axis=0)\n#         pred = np.max(pred, axis=0, keepdims=True)\n        \n#         patient_preds[pidx, :] += post_proc(pred)[0]\n#         del patient_df, patient_paths, dtest, pred\n#         gc.collect()\n        \n#     return patient_preds, np.array(actual_labels)\n\n# # Define a function for post-processing predictions\n# def post_proc(predictions):\n#     proc_pred = np.empty((predictions.shape[0], 2*2 + 3*3), dtype=\"float32\")\n\n#     # bowel, extravasation\n#     proc_pred[:, 0] = predictions[:, 0]\n#     proc_pred[:, 1] = 1 - proc_pred[:, 0]\n#     proc_pred[:, 2] = predictions[:, 1]\n#     proc_pred[:, 3] = 1 - proc_pred[:, 2]\n    \n#     # liver, kidney, spleen\n#     proc_pred[:, 4:7] = predictions[:, 2:5]\n#     proc_pred[:, 7:10] = predictions[:, 5:8]\n#     proc_pred[:, 10:13] = predictions[:, 8:11]\n\n#     return proc_pred\n\n# # Define a function to plot ROC or Precision-Recall curves, training, and validation plots\n# def plot_curves(history, config, data, model, curve_type=\"roc\", plot_accuracy=True):\n# # Create a 5x2 grid for the subplots\n# fig, axes = plt.subplots(5, 2, figsize=(10, 15))  # Change to 5x2 grid\n\n# # Iterate through the metrics and plot them\n# for i, metric_name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n#     # Plot training accuracy\n#     if plot_accuracy:\n#         training_acc_key = f'{metric_name}_accuracy'\n#         validation_acc_key = f'val_{metric_name}_accuracy'\n        \n#         # Use axes to plot instead of creating new subplot\n#         ax = axes[i, 0]  # Use first column for accuracy\n#         ax.plot(history.history[training_acc_key], label='Training ' + metric_name)\n#         ax.plot(history.history[validation_acc_key], label='Validation ' + metric_name)\n#         ax.set_title(metric_name + ' Accuracy')\n#         ax.set_xlabel('Epoch')\n#         ax.set_ylabel('Accuracy')\n#         ax.legend()\n#         ax.grid(True)  # Add gridlines for better readability\n        \n#         # If you also have loss data, plot it in the second column\n#         training_loss_key = f'{metric_name}_loss'\n#         validation_loss_key = f'val_{metric_name}_loss'\n        \n#         ax = axes[i, 1]  # Use second column for loss\n#         ax.plot(history.history[training_loss_key], label='Training ' + metric_name)\n#         ax.plot(history.history[validation_loss_key], label='Validation ' + metric_name)\n#         ax.set_title(metric_name + ' Loss')\n#         ax.set_xlabel('Epoch')\n#         ax.set_ylabel('Loss')\n#         ax.legend()\n#         ax.grid(True)  # Add gridlines for better readability\n\n#     plt.tight_layout()\n\n    \n#     # Getting unique patient IDs from test dataset\n#     predicted_labels, actual_labels = get_predictions_and_labels_for_test(data, model)\n    \n#     for i, metric_name in enumerate(config.TARGET_COLS):\n#         plt.figure(figsize=(10, 7))\n        \n#         if curve_type == \"roc\":\n#             # Compute ROC curve\n#             fpr, tpr, _ = roc_curve(actual_labels[:, i], predicted_labels[:, i])\n#             auc_value = auc(fpr, tpr)\n\n#             # Plot ROC curve\n#             plt.plot(fpr, tpr, lw=2, label=f'Metric: {metric_name}, AUC: {auc_value:.4f}')\n#             plt.xlabel('False Positive Rate')\n#             plt.ylabel('True Positive Rate')\n#             plt.title(f'ROC Curve for {metric_name}')\n#             plt.legend(loc=\"lower right\")\n#         elif curve_type == \"prc\":\n#             # Compute Precision-Recall curve\n#             precision, recall, _ = precision_recall_curve(actual_labels[:, i], predicted_labels[:, i])\n#             auc_value = auc(recall, precision)\n\n#             # Plot Precision-Recall curve\n#             plt.plot(recall, precision, lw=2, label=f'Metric: {metric_name}, AUC: {auc_value:.4f}')\n#             plt.xlabel('Recall')\n#             plt.ylabel('Precision')\n#             plt.title(f'Precision-Recall Curve for {metric_name}')\n#             plt.legend(loc=\"lower left\")\n    \n#     plt.show()\n\n# # Finally, plot ROC or Precision-Recall curves, training, and validation plots\n# plot_curves(history, config, valid_data, model, curve_type=\"roc\", plot_accuracy=True)  # or curve_type=\"prc\" and plot_accuracy=False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the metrics for each head","metadata":{}},{"cell_type":"code","source":"# Create a 3x2 grid for the subplots\nfig, axes = plt.subplots(5, 1, figsize=(5, 15))\n\n# Flatten axes to iterate through them\naxes = axes.flatten()\n\n# Iterate through the metrics and plot them\nfor i, name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n    # Plot training accuracy\n    axes[i].plot(history.history[name + '_accuracy'], label='Training ' + name)\n    # Plot validation accuracy\n    axes[i].plot(history.history['val_' + name + '_accuracy'], label='Validation ' + name)\n    axes[i].set_title(name)\n    axes[i].set_xlabel('Epoch')\n    axes[i].set_ylabel('Accuracy')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the validation loss vs training loss","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store best results\nbest_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nbest_acc_bowel = history.history['val_bowel_accuracy'][best_epoch]\nbest_acc_extra = history.history['val_extra_accuracy'][best_epoch]\nbest_acc_liver = history.history['val_liver_accuracy'][best_epoch]\nbest_acc_kidney = history.history['val_kidney_accuracy'][best_epoch]\nbest_acc_spleen = history.history['val_spleen_accuracy'][best_epoch]\n\n# Find mean accuracy\nbest_acc = np.mean(\n    [best_acc_bowel,\n     best_acc_extra,\n     best_acc_liver,\n     best_acc_kidney,\n     best_acc_spleen\n])\n\n\nprint(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST Acc   : {best_acc:.3f}\\n>>>> BEST Epoch : {best_epoch}\\n')\nprint('ORGAN Acc:')\nprint(f'  >>>> {\"Bowel\".ljust(15)} : {best_acc_bowel:.3f}')\nprint(f'  >>>> {\"Extravasation\".ljust(15)} : {best_acc_extra:.3f}')\nprint(f'  >>>> {\"Liver\".ljust(15)} : {best_acc_liver:.3f}')\nprint(f'  >>>> {\"Kidney\".ljust(15)} : {best_acc_kidney:.3f}')\nprint(f'  >>>> {\"Spleen\".ljust(15)} : {best_acc_spleen:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Store the model for inference","metadata":{}},{"cell_type":"code","source":"# Save the model\nmodel.save(\"rsna-atd.keras\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to S3\ns3resource=boto3.client('s3','ap-southeast-2')\ns3resource.upload_file(\"/kaggle/working/rsna-atd.keras\",\"rsna-kaggle-2023\",\"rsna-atd.keras\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next Steps\n\n1. Please refer to the [Inference Notebook](https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-infer) to learn about submitting to the competition\n2. Dive deep into [KerasCV](https://github.com/keras-team/keras-cv) and [KerasCore](https://github.com/keras-team/keras-core)\n\n# Credits\n\nThis notebook was forked from https://www.kaggle.com/code/awsaf49/rsna-atd-cnn-tpu-train","metadata":{}}]}